{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29c07d78-076a-4274-a5d3-0ab6169d8652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: QSVM-like variational classifier on Iris (binary)\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0679f03a-0786-415e-b79c-cddc502e0b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load binary Iris (setosa vs versicolor)\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "# choose classes 0 and 1 only\n",
    "mask = y < 2\n",
    "X = X[mask][:, :2]  # pick first two features for simplicity (2 features -> 2 qubits)\n",
    "y = y[mask]\n",
    "y = (y == 1).astype(int)  # 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3993eb3e-87ff-4388-b664-9796522dda9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ed29e6c-61ba-4b99-968e-ec821f626680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "299f5354-64a8-4dd1-941d-3a6a209bdba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "n_qubits = 2\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "# Proposal A: angle-encoding + shallow variational layers\n",
    "def feature_map_A(x):\n",
    "    # x has length 2\n",
    "    for i in range(n_qubits):\n",
    "        qml.RX(x[i], wires=i)\n",
    "\n",
    "def variational_layer_A(params):\n",
    "    # params shape (n_qubits,)\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY(params[i], wires=i)\n",
    "    # entangle\n",
    "    qml.CNOT(wires=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8d164ce-a573-446e-9772-709b42bc139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev)\n",
    "def circuit_A(x, params):\n",
    "    feature_map_A(x)\n",
    "    variational_layer_A(params)\n",
    "    return qml.expval(qml.PauliZ(0))  # use expectation of Z on wire 0 as decision \n",
    "\n",
    "# Prediction helper\n",
    "def predict_A(X, params):\n",
    "    preds = []\n",
    "    for x in X:\n",
    "        out = circuit_A(x, params)\n",
    "        preds.append(0 if out > 0 else 1)\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4df9640b-6a77-436a-b3ee-58c506a17214",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'ArrayBox'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m     loss_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean([y_train[i] \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mmax\u001b[39m(probs[i],\u001b[38;5;241m1e-8\u001b[39m)) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39my_train[i]) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mprobs[i],\u001b[38;5;241m1e-8\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_train))])\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(loss_val)\n\u001b[1;32m---> 14\u001b[0m params_A \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mstep(loss, params_A)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ep \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss(params_A)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\main\\Lib\\site-packages\\pennylane\\optimize\\gradient_descent.py:93\u001b[0m, in \u001b[0;36mGradientDescentOptimizer.step\u001b[1;34m(self, objective_fn, grad_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, objective_fn, \u001b[38;5;241m*\u001b[39margs, grad_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Update trainable arguments with one step of the optimizer.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m        If single arg is provided, list [array] is replaced by array.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m     g, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_grad(objective_fn, args, kwargs, grad_fn\u001b[38;5;241m=\u001b[39mgrad_fn)\n\u001b[0;32m     94\u001b[0m     new_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_grad(g, args)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# unwrap from list if one argument, cleaner return\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\main\\Lib\\site-packages\\pennylane\\optimize\\gradient_descent.py:122\u001b[0m, in \u001b[0;36mGradientDescentOptimizer.compute_grad\u001b[1;34m(objective_fn, args, kwargs, grad_fn)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the gradient of the objective function at the given point and return it along with\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03mthe objective function forward pass (if available).\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    will not be evaluated and instead ``None`` will be returned.\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    121\u001b[0m g \u001b[38;5;241m=\u001b[39m get_gradient(objective_fn) \u001b[38;5;28;01mif\u001b[39;00m grad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m grad_fn\n\u001b[1;32m--> 122\u001b[0m grad \u001b[38;5;241m=\u001b[39m g(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    123\u001b[0m forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(g, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    125\u001b[0m num_trainable_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)\n",
      "File \u001b[1;32m~\\.conda\\envs\\main\\Lib\\site-packages\\pennylane\\_grad.py:318\u001b[0m, in \u001b[0;36mgrad.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ()\n\u001b[1;32m--> 318\u001b[0m grad_value, ans \u001b[38;5;241m=\u001b[39m grad_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward \u001b[38;5;241m=\u001b[39m ans\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad_value\n",
      "File \u001b[1;32m~\\.conda\\envs\\main\\Lib\\site-packages\\autograd\\wrap_util.py:23\u001b[0m, in \u001b[0;36munary_to_nary.<locals>.nary_operator.<locals>.nary_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(args[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m argnum)\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unary_operator(unary_f, x, \u001b[38;5;241m*\u001b[39mnary_op_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnary_op_kwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\main\\Lib\\site-packages\\pennylane\\_grad.py:336\u001b[0m, in \u001b[0;36mgrad._grad_with_forward\u001b[1;34m(fun, x)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;129m@unary_to_nary\u001b[39m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_grad_with_forward\u001b[39m(fun, x):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This function is a replica of ``autograd.grad``, with the only\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    difference being that it returns both the gradient *and* the forward pass\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;124;03m    value.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 336\u001b[0m     vjp, ans \u001b[38;5;241m=\u001b[39m _make_vjp(fun, x)  \u001b[38;5;66;03m# pylint: disable=redefined-outer-name\u001b[39;00m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m vspace(ans)\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    340\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrad only applies to real scalar-output functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    341\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry jacobian, elementwise_grad or holomorphic_grad.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    342\u001b[0m         )\n",
      "File \u001b[1;32m~\\.conda\\envs\\main\\Lib\\site-packages\\autograd\\core.py:12\u001b[0m, in \u001b[0;36mmake_vjp\u001b[1;34m(fun, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_vjp\u001b[39m(fun, x):\n\u001b[0;32m     11\u001b[0m     start_node \u001b[38;5;241m=\u001b[39m VJPNode\u001b[38;5;241m.\u001b[39mnew_root()\n\u001b[1;32m---> 12\u001b[0m     end_value, end_node \u001b[38;5;241m=\u001b[39m trace(start_node, fun, x)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end_node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvjp\u001b[39m(g):\n",
      "File \u001b[1;32m~\\.conda\\envs\\main\\Lib\\site-packages\\autograd\\tracer.py:12\u001b[0m, in \u001b[0;36mtrace\u001b[1;34m(start_node, fun, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace_stack\u001b[38;5;241m.\u001b[39mnew_trace() \u001b[38;5;28;01mas\u001b[39;00m t:\n\u001b[0;32m     11\u001b[0m     start_box \u001b[38;5;241m=\u001b[39m new_box(x, t, start_node)\n\u001b[1;32m---> 12\u001b[0m     end_box \u001b[38;5;241m=\u001b[39m fun(start_box)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isbox(end_box) \u001b[38;5;129;01mand\u001b[39;00m end_box\u001b[38;5;241m.\u001b[39m_trace \u001b[38;5;241m==\u001b[39m start_box\u001b[38;5;241m.\u001b[39m_trace:\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m end_box\u001b[38;5;241m.\u001b[39m_value, end_box\u001b[38;5;241m.\u001b[39m_node\n",
      "File \u001b[1;32m~\\.conda\\envs\\main\\Lib\\site-packages\\autograd\\wrap_util.py:17\u001b[0m, in \u001b[0;36munary_to_nary.<locals>.nary_operator.<locals>.nary_f.<locals>.unary_f\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     subargs \u001b[38;5;241m=\u001b[39m subvals(args, \u001b[38;5;28mzip\u001b[39m(argnum, x))\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;241m*\u001b[39msubargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m, in \u001b[0;36mloss\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m     10\u001b[0m probs \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m val) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m preds]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# binary cross-entropy\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean([y_train[i] \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mmax\u001b[39m(probs[i],\u001b[38;5;241m1e-8\u001b[39m)) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39my_train[i]) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mprobs[i],\u001b[38;5;241m1e-8\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_train))])\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(loss_val)\n",
      "File \u001b[1;32m~\\.conda\\envs\\main\\Lib\\site-packages\\pennylane\\numpy\\wrapper.py:117\u001b[0m, in \u001b[0;36mtensor_wrapper.<locals>._wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m         tensor_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _np\u001b[38;5;241m.\u001b[39many([i\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tensor_args])\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# evaluate the original object\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m res \u001b[38;5;241m=\u001b[39m obj(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, _np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# only if the output of the object is a ndarray,\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# then convert to a PennyLane tensor\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     res \u001b[38;5;241m=\u001b[39m tensor(res, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtensor_kwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\main\\Lib\\site-packages\\autograd\\tracer.py:54\u001b[0m, in \u001b[0;36mprimitive.<locals>.f_wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_box(ans, trace, node)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f_raw(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\main\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3904\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3902\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 3904\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _methods\u001b[38;5;241m.\u001b[39m_mean(a, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   3905\u001b[0m                       out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\main\\Lib\\site-packages\\numpy\\_core\\_methods.py:147\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    145\u001b[0m         ret \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype(ret \u001b[38;5;241m/\u001b[39m rcount)\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m         ret \u001b[38;5;241m=\u001b[39m ret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype(ret \u001b[38;5;241m/\u001b[39m rcount)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     ret \u001b[38;5;241m=\u001b[39m ret \u001b[38;5;241m/\u001b[39m rcount\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# Train A (simple gradient descent)\n",
    "init_params_A = np.random.randn(n_qubits) * 0.1\n",
    "opt = qml.GradientDescentOptimizer(stepsize=0.2)\n",
    "params_A = init_params_A.copy()\n",
    "epochs = 50\n",
    "for ep in range(epochs):\n",
    "    def loss(p):\n",
    "        preds = [circuit_A(x, p) for x in X_train]\n",
    "        # map expectation (in [-1,1]) to probabilities: use (1 - exp)/2 as prob of label 1\n",
    "        probs = [(1 - val) / 2 for val in preds]\n",
    "        # binary cross-entropy\n",
    "        loss_val = -np.mean([y_train[i] * np.log(max(probs[i],1e-8)) + (1-y_train[i]) * np.log(max(1-probs[i],1e-8)) for i in range(len(y_train))])\n",
    "        return float(loss_val)\n",
    "    params_A = opt.step(loss, params_A)\n",
    "    if ep % 10 == 0:\n",
    "        print(f\"Epoch {ep}, loss {loss(params_A):.4f}\")\n",
    "acc_A_train = np.mean(predict_A(X_train, params_A) == y_train)\n",
    "acc_A_test = np.mean(predict_A(X_test, params_A) == y_test)\n",
    "print(\"Proposal A - train acc:\", acc_A_train, \"test acc:\", acc_A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4acf146a-9907-45c7-832c-920799c2e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proposal B: feature-map with 2-qubit interaction + deeper variational layers\n",
    "layers = 2\n",
    "def feature_map_B(x):\n",
    "    # encode non-linear features via entangling rotations\n",
    "    qml.RZ(x[0], wires=0)\n",
    "    qml.RZ(x[1], wires=1)\n",
    "    qml.CNOT(wires=[0,1])\n",
    "    qml.RX(x[0]*x[1], wires=0)\n",
    "\n",
    "def variational_layer_B(params, layer):\n",
    "    # params has shape (layers, n_qubits, 2) for RY and RZ\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY(params[layer,i,0], wires=i)\n",
    "        qml.RZ(params[layer,i,1], wires=i)\n",
    "    # entanglement ring\n",
    "    qml.CNOT(wires=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ae95bd9-dcf7-4c34-8a27-9b718ac9c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev)\n",
    "def circuit_B(x, params):\n",
    "    feature_map_B(x)\n",
    "    for l in range(params.shape[0]):\n",
    "        variational_layer_B(params, l)\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "# predict by combining expectations\n",
    "def predict_B(X, params):\n",
    "    preds = []\n",
    "    for x in X:\n",
    "        exps = circuit_B(x, params)\n",
    "        # simple combiner\n",
    "        score = exps[0] - exps[1]\n",
    "        preds.append(0 if score > 0 else 1)\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe3fbe37-ba17-4de2-8442-77e2e40318b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EpochB 0, loss 1.0807\n",
      "EpochB 10, loss 0.9918\n",
      "EpochB 20, loss 0.9829\n",
      "EpochB 30, loss 0.9759\n",
      "EpochB 40, loss 0.9754\n",
      "EpochB 50, loss 0.9752\n",
      "Proposal B - train acc: 0.6285714285714286 test acc: 0.4666666666666667\n"
     ]
    }
   ],
   "source": [
    "# initialize params\n",
    "params_B = np.random.randn(layers, n_qubits, 2) * 0.1\n",
    "optB = qml.AdamOptimizer(0.1)\n",
    "epochs = 60\n",
    "for ep in range(epochs):\n",
    "    def lossB(p):\n",
    "        preds = [predict_B([x], p)[0] for x in X_train]  # not differentiable; instead use expectation-based loss\n",
    "        # better: use raw expectations as logits\n",
    "        outs = [circuit_B(x, p) for x in X_train]\n",
    "        # map to score and use MSE with labels\n",
    "        scores = [out[0] - out[1] for out in outs]\n",
    "        return np.mean((np.array(scores) - (1-2*y_train))**2)\n",
    "    params_B = optB.step(lossB, params_B)\n",
    "    if ep % 10 == 0:\n",
    "        print(f\"EpochB {ep}, loss {lossB(params_B):.4f}\")\n",
    "\n",
    "acc_B_train = np.mean(predict_B(X_train, params_B) == y_train)\n",
    "acc_B_test = np.mean(predict_B(X_test, params_B) == y_test)\n",
    "print(\"Proposal B - train acc:\", acc_B_train, \"test acc:\", acc_B_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "930b8ab4-4afe-4a7d-b48d-37acf021cc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick expressibility probe: sample random params and compute pairwise state fidelities\n",
    "def expressibility_probe(n_samples=50):\n",
    "    rand_params = np.random.randn(n_samples, layers, n_qubits, 2)\n",
    "    states = []\n",
    "    for p in rand_params:\n",
    "        # get statevector\n",
    "        @qml.qnode(dev)\n",
    "        def s():\n",
    "            feature_map_B([0.1, 0.2])  # irrelevant static input\n",
    "            for l in range(p.shape[0]):\n",
    "                variational_layer_B(p, l)\n",
    "            return qml.state()\n",
    "        states.append(s())\n",
    "    # compute pairwise fidelities\n",
    "    F = []\n",
    "    for i in range(len(states)):\n",
    "        for j in range(i+1, len(states)):\n",
    "            F.append(np.abs(np.vdot(states[i], states[j]))**2)\n",
    "    F = np.array(F)\n",
    "    print(\"Expressibility probe -> mean fidelity:\", np.mean(F), \"std:\", np.std(F))\n",
    "    return np.mean(F), np.std(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d8364a2-35ee-4279-b6db-26217736bd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expressibility probe -> mean fidelity: 0.3593606964319812 std: 0.23233766980746454\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    expressibility_probe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491a4271-d489-401b-b90c-06757baf2066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
